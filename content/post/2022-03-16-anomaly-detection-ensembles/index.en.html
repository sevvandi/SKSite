---
title: Anomaly Detection Ensembles
author: Sevvandi Kandanaarachchi
date: '2022-03-16'
slug: []
categories: []
tags: []
subtitle: ''
summary: ''
authors: []
lastmod: '2022-03-16T15:50:32+11:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

<script src="{{< blogdown/postref >}}index.en_files/header-attrs/header-attrs.js"></script>


<div id="what-is-an-anomaly-detection-ensemble" class="section level2">
<h2>What is an anomaly detection ensemble?</h2>
<p>It is a bunch of anomaly detection methods put together to get a final anomaly score/prediction. So you have a bunch of methods, and each of these methods have its own anomaly score, which is used by the ensemble to come up with the consensus score.</p>
<p>What are the ways of constructing an anomaly detection ensemble? Broadly, anomaly detection ensembles can be categorised into 3 camps.</p>
<ol style="list-style-type: decimal">
<li>Feature bagging</li>
<li>Subsampling</li>
<li>Using combination functions</li>
</ol>
</div>
<div id="feature-bagging" class="section level2">
<h2>Feature bagging</h2>
<p>Feature bagging uses different attribute subsets to find anomalies. In a dataset, generally observations are denoted by rows and attributes are denoted by columns. Feature bagging considers different column subsets. That is, multiple copies of the same dataset each having a slightly different set of columns is considered. For each dataset copy, we find anomalies using a single anomaly detection method. Then the anomaly scores are averaged to compute the ensemble score.</p>
<p>Let us try this with the letter dataset from the <a href="http://odds.cs.stonybrook.edu/">ODDS repository</a>. We first read the dataset and normalize it so that each column has values within 0 and 1. Let’s have a loot at the data after normalising.</p>
<pre class="r"><code>datori &lt;- readMat(&quot;letter.mat&quot;)
Xori &lt;- datori$X
Xori &lt;- unitize(Xori)
head(Xori)</code></pre>
<pre><code>##            [,1]      [,2]       [,3] [,4]       [,5] [,6]      [,7]       [,8]
## [1,] 0.40000000 0.6666667 0.33333333  0.6 0.20000000  0.7 0.3076923 0.30769231
## [2,] 0.00000000 0.4000000 0.00000000  0.4 0.00000000  0.4 0.3846154 0.30769231
## [3,] 0.26666667 0.4666667 0.33333333  0.5 0.20000000  0.4 0.4615385 0.15384615
## [4,] 0.06666667 0.4000000 0.06666667  0.4 0.13333333  0.4 0.3846154 0.00000000
## [5,] 0.06666667 0.1333333 0.06666667  0.3 0.06666667  0.4 0.3846154 0.07692308
## [6,] 0.06666667 0.3333333 0.00000000  0.7 0.00000000  0.4 0.3846154 0.30769231
##      [,9] [,10]     [,11]     [,12]     [,13]     [,14]     [,15]     [,16]
## [1,]  0.6   1.0 0.1818182 0.3636364 0.1333333 0.6428571 0.3636364 0.8888889
## [2,]  0.4   0.3 0.4545455 0.5454545 0.0000000 0.5714286 0.0000000 0.5555556
## [3,]  0.7   0.3 0.4545455 0.6363636 0.0000000 0.5714286 0.3636364 0.5555556
## [4,]  0.7   0.3 0.4545455 0.5454545 0.0000000 0.5714286 0.2727273 0.5555556
## [5,]  0.7   0.3 0.4545455 0.5454545 0.0000000 0.5714286 0.2727273 0.5555556
## [6,]  0.4   0.3 0.4545455 0.5454545 0.0000000 0.5714286 0.0000000 0.5555556
##           [,17]     [,18]      [,19]     [,20]      [,21] [,22]     [,23]
## [1,] 0.26666667 0.6666667 0.33333333 0.8888889 0.14285714   0.5 0.5714286
## [2,] 0.20000000 0.4666667 0.26666667 0.5555556 0.14285714   0.5 0.4285714
## [3,] 0.06666667 0.2666667 0.00000000 0.2222222 0.00000000   0.5 0.4285714
## [4,] 0.13333333 0.1333333 0.06666667 0.3333333 0.07142857   0.5 0.4285714
## [5,] 0.06666667 0.2666667 0.06666667 0.3333333 0.07142857   0.5 0.4285714
## [6,] 0.06666667 0.6666667 0.00000000 0.7777778 0.00000000   0.5 0.4285714
##           [,24]     [,25]     [,26] [,27]     [,28] [,29]     [,30]      [,31]
## [1,] 0.00000000 0.5714286 1.0000000   0.5 0.3076923     0 0.7500000 0.18181818
## [2,] 0.00000000 0.5000000 0.9285714   0.5 0.4615385     0 0.5833333 0.09090909
## [3,] 0.07142857 0.5000000 0.5000000   0.5 0.4615385     0 0.5833333 0.18181818
## [4,] 0.07142857 0.5714286 0.5000000   0.5 0.4615385     0 0.5833333 0.27272727
## [5,] 0.07142857 0.5714286 0.5000000   0.5 0.5384615     0 0.5833333 0.27272727
## [6,] 0.28571429 0.2857143 0.5000000   0.5 0.4615385     0 0.5833333 0.00000000
##      [,32]
## [1,]   0.5
## [2,]   0.5
## [3,]   0.6
## [4,]   0.6
## [5,]   0.6
## [6,]   0.6</code></pre>
<p>Now, feature bagging would select different column subsets. Let’s pick different columns.</p>
<pre class="r"><code>set.seed(1)
dd &lt;- dim(Xori)[2]
sample_list &lt;- list()
for(i in 1:10){
  sample_list[[i]] &lt;- sample(1:dd, 20)
}
sample_list[[1]]</code></pre>
<pre><code>##  [1] 25  4  7  1  2 23 11 14 18 19 29 21 10 32 20 30  9 15  5 27</code></pre>
<pre class="r"><code>sample_list[[2]]</code></pre>
<pre><code>##  [1]  9 25 14  5 29  2 10 31 12 15  1 20  3  6 26 18 19 23  4 24</code></pre>
<p>Next we select the subset of columns in each sample_list and find anomalies in each subsetted-dataset. Let’s use the KNN_AGG anomaly detection method. This method aggregates the k-nearest neighbour distances. If a data point has high KNN distances compared to other points, then it is considered anomalous, because it is far away from other points.</p>
<pre class="r"><code>library(DDoutlier)
knn_scores &lt;- matrix(0, nrow = NROW(Xori), ncol = 10)
for(i in 1:10){
  knn_scores[ ,i] &lt;- KNN_AGG(Xori[ ,sample_list[[i]]])
}
head(knn_scores)</code></pre>
<pre><code>##           [,1]      [,2]      [,3]      [,4]      [,5]      [,6]      [,7]
## [1,] 21.635185 22.270694 24.987923 20.026546 22.158682 22.422737 20.455542
## [2,]  9.969831  9.661039  9.432406  5.255449  5.540366  7.325621  9.302664
## [3,] 16.061851 20.796525 11.832477 14.235700 11.481942 14.899982 13.096920
## [4,] 12.409446 14.012729 11.400507  7.347249  7.739870  9.309485  9.793680
## [5,]  9.998477 12.541672 10.209202  6.857942  5.919889  9.355743  8.214986
## [6,] 12.350209 12.350209  7.409608  4.905166  5.045559  3.310723  7.461390
##          [,8]      [,9]     [,10]
## [1,] 23.55476 21.493597 25.954645
## [2,] 10.51396  7.460010  8.571801
## [3,] 17.98096  9.872211 19.625926
## [4,] 11.78520  7.757524 11.292598
## [5,] 12.24067  6.955316 11.085984
## [6,] 12.29468  3.638703  7.127627</code></pre>
<p>Now we have the anomaly scores for the 10 subsetted-datasets. In feature bagging the general method of consensus is to add up the scores or take the mean of the scores, which is an equivalent thing to do.</p>
<pre class="r"><code>bagged_score &lt;- apply(knn_scores, 1, mean)</code></pre>
<p>We can compare the bagged anomaly scores with the anomaly scores f we didn’t use bagging. That is, if we used the full dataset, what would be anomaly scores? Does bagging make it better? For this we need the labels/ground truth. To evaluate the performance, we use the area under the ROC curve.</p>
<pre class="r"><code>library(pROC)
labels &lt;- datori$y[ ,1]

# anomaly scores without feature bagging - taking the full dataset
knn_agg_without &lt;- KNN_AGG(Xori)

# ROC  - without bagging
rocobj1 &lt;- roc(labels, knn_agg_without, direction = &quot;&lt;&quot;)
rocobj1$auc</code></pre>
<pre><code>## Area under the curve: 0.9097</code></pre>
<pre class="r"><code>rocobj2 &lt;- roc(labels, bagged_score, direction = &quot;&lt;&quot;)
rocobj2$auc</code></pre>
<pre><code>## Area under the curve: 0.9101</code></pre>
<p>Yes! We see that there is an increase in AUC (area under the ROC curve) by feature bagging. In this case it is a small improvement. But, nonetheless there is an improvement.</p>
</div>
