<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Sevvandi Kandanaarachchi</title>
    <link>/post/</link>
      <atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sat, 29 Aug 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/icon-192.png</url>
      <title>Posts</title>
      <link>/post/</link>
    </image>
    
    <item>
      <title>Which nonlinear dimension reduction methods can detect outliers inside a sphere?</title>
      <link>/post/2020-08-30-nonlinear-dimension-reduction-and-outlier-detection/</link>
      <pubDate>Sat, 29 Aug 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-08-30-nonlinear-dimension-reduction-and-outlier-detection/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;In this post we will look at how well nonlinear dimension reduction techniques detect outliers that are placed inside a sphere, when all the other data points are on the surface of the sphere. We will use the R package &lt;strong&gt;dimRed&lt;/strong&gt; for this analysis. First we note that linear project-based methods on the original data will not work, because projections will hide the outliers inside the sphere. Also, none of the nonlinear dimension reduction methods we consider are especially designed for outlier detection. So, it is not a limitation of the method if they do not detect outliers. But, we want to see if any of them do.&lt;/p&gt;
&lt;p&gt;Let’s plot the data first.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
theta &amp;lt;- seq(from = -1*pi, to = pi, by=0.01)
phi &amp;lt;- seq(from = 0, to= pi*2, by=0.01)
theta1 &amp;lt;- sample(theta, size=5*length(theta), replace=TRUE)
phi1 &amp;lt;- sample(phi, size=5*length(phi), replace=TRUE)
x &amp;lt;- cos(theta1)*cos(phi1)
y &amp;lt;- cos(theta1)*sin(phi1)
z &amp;lt;- sin(theta1)

df &amp;lt;- cbind.data.frame(x,y,z)
df1 &amp;lt;- df
dim(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3145    3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;oo &amp;lt;- matrix(c(0,0,0,0,0.2,-0.1), nrow=2, byrow = TRUE)
colnames(oo) &amp;lt;- colnames(df)
df &amp;lt;- rbind.data.frame(df, oo)
plot(df[ ,c(1,2)], pch=20)
points(df[3146:3147, ], pch=20, col=c(&amp;quot;red&amp;quot;, &amp;quot;green&amp;quot;), cex=2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-08-30-nonlinear-dimension-reduction-and-outlier-detection/index_files/figure-html/dataset-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dd2 &amp;lt;- dimRedData(df)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The dataframe contains 3145 points on the surface of the sphere. These points are non-outliers. The data points in the dataframe on rows 3146 and 3147 are the outliers. We plot the outliers in red and green.&lt;/p&gt;
&lt;p&gt;Most methods need a parameter such as &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; in KNN distances. For all methods we fix &lt;span class=&#34;math inline&#34;&gt;\(k=10\)&lt;/span&gt; and map the original data from &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^3\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^2\)&lt;/span&gt;. It is also called a 2-dimensional embedding. Let’s start our analysis with IsoMap.&lt;/p&gt;
&lt;div id=&#34;isomap&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;IsoMap&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;emb2 &amp;lt;- embed(dd2, &amp;quot;Isomap&amp;quot;, .mute = NULL, knn = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2020-08-30 20:39:23: Isomap START&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2020-08-30 20:39:23: constructing knn graph&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2020-08-30 20:39:23: calculating geodesic distances&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2020-08-30 20:39:28: Classical Scaling&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;embdat &amp;lt;- as.data.frame(emb2@data)
plot(embdat, pch=20,main=&amp;quot;IsoMap embedding&amp;quot;)
points(embdat[3146:3147, ],  pch=20, col=c(&amp;quot;red&amp;quot;, &amp;quot;green&amp;quot;), cex=2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-08-30-nonlinear-dimension-reduction-and-outlier-detection/index_files/figure-html/isomap-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Well, &lt;span class=&#34;math inline&#34;&gt;\(k=10\)&lt;/span&gt;, did not bring out the outliers inside the sphere using IsoMap. Next, let’s look at Locally Linear Embedding (LLE).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lle&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;LLE&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;emb2 &amp;lt;- embed(dd2, &amp;quot;LLE&amp;quot;, knn = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## finding neighbours
## calculating weights
## computing coordinates&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;embdat &amp;lt;- as.data.frame(emb2@data)
plot(embdat, pch=20, main=&amp;quot;LLE embedding&amp;quot;)
points(embdat[3146:3147, ], pch=20, col=c(&amp;quot;red&amp;quot;, &amp;quot;green&amp;quot;), cex=2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-08-30-nonlinear-dimension-reduction-and-outlier-detection/index_files/figure-html/LLEfun-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Next we look at Laplacian Eigenmaps.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;laplacian-eigenmaps&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Laplacian Eigenmaps&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;emb2 &amp;lt;- embed(dd2, &amp;quot;LaplacianEigenmaps&amp;quot;, knn = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2020-08-30 20:42:38: Creating weight matrix&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2020-08-30 20:42:39: Eigenvalue decomposition&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Eigenvalues: 5.098969e-03 3.829450e-03 4.537894e-17&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2020-08-30 20:42:39: DONE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;embdat &amp;lt;- as.data.frame(emb2@data)
plot(embdat, pch=20, main=&amp;quot;Lapacian eigenmaps embedding&amp;quot;)
points(embdat[3146:3147, ], pch=20, col=c(&amp;quot;red&amp;quot;, &amp;quot;green&amp;quot;), cex=2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-08-30-nonlinear-dimension-reduction-and-outlier-detection/index_files/figure-html/laplace-1.png&#34; width=&#34;672&#34; /&gt;
This is nice. Laplacian eigenmaps really brought out the outliers using &lt;span class=&#34;math inline&#34;&gt;\(k=10\)&lt;/span&gt;. Next we look at diffusion maps.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;diffusion-maps&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Diffusion Maps&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;emb2 &amp;lt;- embed(dd2, &amp;quot;DiffusionMaps&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Performing eigendecomposition
## Computing Diffusion Coordinates
## Elapsed time: 47.7 seconds&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;embdat &amp;lt;- as.data.frame(emb2@data)
plot(embdat, pch=20, main=&amp;quot;Diffusion maps embedding&amp;quot;)
points(embdat[3146:3147, ],  pch=20, col=c(&amp;quot;red&amp;quot;, &amp;quot;green&amp;quot;), cex=2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-08-30-nonlinear-dimension-reduction-and-outlier-detection/index_files/figure-html/diffusion-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Diffusion maps also brought out the outliers. Interestingly, everything else is mapped to a line. Next we consider non-metric dimensional scaling.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;non-metric-dimensional-scaling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Non-Metric Dimensional Scaling&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;emb2 &amp;lt;- embed(dd2, &amp;quot;nMDS&amp;quot;, d = function(x) exp(dist(x)))

embdat &amp;lt;- as.data.frame(emb2@data)
plot(embdat, pch=20, main=&amp;quot;non-MDS embedding&amp;quot;)
points(embdat[3146:3147, ], pch=20, col=c(&amp;quot;red&amp;quot;, &amp;quot;green&amp;quot;), cex=2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-08-30-nonlinear-dimension-reduction-and-outlier-detection/index_files/figure-html/nonmetric-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Finally we do tsne.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tsne&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;tsne&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;emb2 &amp;lt;- embed(dd2, &amp;quot;tSNE&amp;quot;, perplexity = 10)

embdat &amp;lt;- as.data.frame(emb2@data)
plot(embdat, pch=20, main=&amp;quot;tsne embedding&amp;quot;)
points(embdat[3146:3147, ], pch=20, col=c(&amp;quot;red&amp;quot;, &amp;quot;green&amp;quot;), cex=2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-08-30-nonlinear-dimension-reduction-and-outlier-detection/index_files/figure-html/tsne-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Of the methods explored, Laplacian eigenmaps brought out the outliers while showing some spherical structure in the embedding. Diffusion maps also brought out the outliers. However, the spherical structure of the data was lost in the embedding.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Anomaly detection dilemmas</title>
      <link>/post/2020-07-19-anomaly-detection-datasets/</link>
      <pubDate>Sun, 19 Jul 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-07-19-anomaly-detection-datasets/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Finding anomalies/outliers in data is a task that is increasingly getting more attention mainly due to the variety of applications involved. Not that it is a new field of research. Rather, outlier/anomaly detection has been studied by statisticians and computer scientists for a long time. However, there are certain aspects which lack consensus.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;What’s in a name?&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The words outlier/anomaly/novelty/extremes are sometimes used to describe the same thing; sometimes for slightly different things. So, when we read a paper, we need to be careful what the word means, because it may not mean what we think it means.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Lack of definitions&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Antony Unwin in his paper &lt;em&gt;Multivariate outliers and the O3 plot&lt;/em&gt; says &lt;em&gt;“Outliers are a complicated business. It is difficult to define what they are, it is difficult to identify them, and it is difficult to assess how they affect analyses”.&lt;/em&gt; Sometimes we find that the definition of an outlier depends on the application. For example, chromosomal anomalies in tumours may have very different distributional properties when compared with fraudulent credit card transactions among billions of legitimate transactions. This makes it difficult if not impossible for researchers to come up with algorithms that detect anomalies in all situations. Often, parameter selection plays a role. For example, if we are using an anomaly detection method based on knn distances, frequently we’re faced with the question “which k works best?”.&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Identify outliers or give scores?&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Researchers broadly detect outliers in two different ways. (1) Identify outliers, i.e. declare if a data point is an outlier. This is a binary declaration. (2) Give a score of outlyingness. With this method, each point in the dataset gets a score of outlyingness. Both ways have pros and cons. The binary identification of outliers does not tell you how outlying it is. With this technique we have no sense which point is the most outlying of the identified outliers. On the other hand, the scoring method does not tell you which points are actually outliers. That is left to the user. The user can define a threshold and say that points with outlying scores above that threshold are outliers. However, coming up with a meaningful threshold may not be an easy task.&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Evaluation methods&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Again, there is no consensus on how to evaluate anomaly detection methods. This is not surprising given that outlier detection methods have two modes of operation: identify outliers or give scores. Methods that identify outliers evaluate the anomaly detection method by using metrics such as true positive rate, false positive rate, positive predictive power and negative predictive power. On the other hand, anomaly detection methods that give scores use area under the Receiver Operator Characteristic (ROC) curve, or area under the Precision Recall curve to evaluate effectiveness.&lt;/p&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Datasets, or the lack there of&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I believe it is fair to say that until recently there were no benchmark datasets for anomaly detection or at least little was known about such datasets. In the recent past there has been some attempt to fill this gap and now there are some repositories of datasets specially prepared for anomaly detection.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tips for increasing your happiness during self-isolation</title>
      <link>/post/2020-04-13-tips-for-increasing-your-happiness/</link>
      <pubDate>Mon, 13 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-04-13-tips-for-increasing-your-happiness/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Teaching first year students made me realize how difficult it is for young people to stay at home all the time, specially during this COVID-19 crisis. My understanding is that it makes people frustrated and unhappy. How can we support them? Are there tips that can help? Here is a list of things that I think will help to increase your sense of well being. Give it a try! &lt;img src=&#34;featured.JPG&#34; alt=&#34;Alt text&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using dobin for time series data</title>
      <link>/post/dobin-for-time-series/</link>
      <pubDate>Sat, 16 Nov 2019 00:00:00 +0000</pubDate>
      <guid>/post/dobin-for-time-series/</guid>
      <description>


&lt;p&gt;The R package &lt;em&gt;dobin&lt;/em&gt; can be used as a dimension reduction tool for outlier detection. So, if we have a dataset of &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; independent observations, where each observation is of dimension &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, &lt;em&gt;dobin&lt;/em&gt; can be used to find a new basis, such that the outliers of this dataset are highlighted using fewer basis vectors (see &lt;a href=&#34;https://sevvandi.github.io/dobin/index.html&#34;&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;But, how do we use &lt;em&gt;dobin&lt;/em&gt; for time series data? &lt;em&gt;Dobin&lt;/em&gt; is not meant for raw time series data because it is time-dependent. But we can break a time series into consecutive non-overlapping windows and compute features of data in each window using an R package such as &lt;a href=&#34;https://pkg.robjhyndman.com/tsfeatures/&#34;&gt;&lt;em&gt;tsfeatures&lt;/em&gt;&lt;/a&gt;. If we compute &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; features, then data in each time series window will be denoted by a point in &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^d\)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;a-synthetic-example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A Synthetic Example&lt;/h2&gt;
&lt;p&gt;Let’s look at an example. We make a normally distributed time series of length &lt;span class=&#34;math inline&#34;&gt;\(6000\)&lt;/span&gt; and insert an outlier at the position &lt;span class=&#34;math inline&#34;&gt;\(1010\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::opts_chunk$set(cache=TRUE)
library(tsfeatures)
library(dplyr)
library(dobin)
library(ggplot2)

set.seed(1)
# Generate 6000 random normally distributed points for a time series
y &amp;lt;- rnorm(6000)
# Insert an additive outlier at position 1010
y[1010] &amp;lt;- 6
df &amp;lt;- cbind.data.frame(1:6000, y)
colnames(df) &amp;lt;- c(&amp;quot;Index&amp;quot;, &amp;quot;Value&amp;quot;)
ggplot(df, aes(Index, Value)) + geom_point() + theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-11-16-using-dobin-for-time-series/index_files/figure-html/setup-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now, let us break the time series into non-overlapping chunks of length &lt;span class=&#34;math inline&#34;&gt;\(50\)&lt;/span&gt;, i.e. we get &lt;span class=&#34;math inline&#34;&gt;\(120\)&lt;/span&gt; chunks or windows. Why do we use non-overlapping windows? If we use overlapping windows, say sliding by &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;, the outlying point in the time series contributes to &lt;span class=&#34;math inline&#34;&gt;\(50\)&lt;/span&gt; windows. Later, when we compute features of these time series windows, these &lt;span class=&#34;math inline&#34;&gt;\(50\)&lt;/span&gt; windows will have similar features, but they will not be outliers in the feature space, because there are &lt;span class=&#34;math inline&#34;&gt;\(50\)&lt;/span&gt; of them. That is why we use non-overlapping windows.&lt;/p&gt;
&lt;p&gt;Also, note that we need the time series to have a decent length to compute features. For each window we compute time series features using &lt;em&gt;tsfeatures&lt;/em&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Split the time series into windows of length 50
my_data_list &amp;lt;- split(y, rep(1:120, each = 50))
# Compute features of each chunk using tsfeatues
ftrs &amp;lt;- tsfeatures(my_data_list)
head(ftrs)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 16
##   frequency nperiods seasonal_period  trend   spike linearity curvature
##       &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;           &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1         1        0               1 0.0506 1.01e-3     0.354    0.212 
## 2         1        0               1 0.110  6.68e-4    -0.500    0.0679
## 3         1        0               1 0.201  8.10e-4    -2.18    -0.836 
## 4         1        0               1 0.129  5.11e-4    -0.402   -1.57  
## 5         1        0               1 0.134  7.74e-4    -0.817    1.39  
## 6         1        0               1 0.0673 1.06e-3     0.130    0.681 
## # ... with 9 more variables: e_acf1 &amp;lt;dbl&amp;gt;, e_acf10 &amp;lt;dbl&amp;gt;, entropy &amp;lt;dbl&amp;gt;,
## #   x_acf1 &amp;lt;dbl&amp;gt;, x_acf10 &amp;lt;dbl&amp;gt;, diff1_acf1 &amp;lt;dbl&amp;gt;, diff1_acf10 &amp;lt;dbl&amp;gt;,
## #   diff2_acf1 &amp;lt;dbl&amp;gt;, diff2_acf10 &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is easier to find a good set of basis vectors that highlight outliers when there are a lot more points compared to the dimensions of the dataset, i.e. &lt;span class=&#34;math inline&#34;&gt;\(N &amp;gt; p\)&lt;/span&gt;. In this case the feature space is &lt;span class=&#34;math inline&#34;&gt;\(16\)&lt;/span&gt; dimensional, and we have &lt;span class=&#34;math inline&#34;&gt;\(120\)&lt;/span&gt; points, each point corresponding to a window of the time seires.&lt;/p&gt;
&lt;p&gt;Next we input these time series features to &lt;em&gt;dobin&lt;/em&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ftrs %&amp;gt;% dobin(norm=2) -&amp;gt; out
coords &amp;lt;- as.data.frame(out$coords[ ,1:2])
colnames(coords) &amp;lt;- c(&amp;quot;DC1&amp;quot;, &amp;quot;DC2&amp;quot;)
ggplot(coords, aes(DC1, DC2)) + geom_point() + theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-11-16-using-dobin-for-time-series/index_files/figure-html/dobin-1.png&#34; width=&#34;672&#34; /&gt; In the first and second dobin component space (DC1-DC2 space), we see a point appearing far away near &lt;span class=&#34;math inline&#34;&gt;\((15, -5)\)&lt;/span&gt;. Let’s investigate this point.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;inds &amp;lt;- which(coords[ ,1] &amp;gt; 10)
inds&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 21&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;OK, this point is coming from window 21. Also, this point deviates in the DC1 axis. So, let us look at the first dobin vector.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# First dobin vector
out$vec[ ,1]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  0.00000000  0.00000000  0.00000000  0.12507580  0.91723338
##  [6]  0.10686900  0.12483596  0.08128369  0.20790487 -0.08597682
## [11]  0.06804500  0.17399103  0.05037166  0.08260081 -0.06594736
## [16]  0.10098625&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;colnames(ftrs)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;frequency&amp;quot;       &amp;quot;nperiods&amp;quot;        &amp;quot;seasonal_period&amp;quot;
##  [4] &amp;quot;trend&amp;quot;           &amp;quot;spike&amp;quot;           &amp;quot;linearity&amp;quot;      
##  [7] &amp;quot;curvature&amp;quot;       &amp;quot;e_acf1&amp;quot;          &amp;quot;e_acf10&amp;quot;        
## [10] &amp;quot;entropy&amp;quot;         &amp;quot;x_acf1&amp;quot;          &amp;quot;x_acf10&amp;quot;        
## [13] &amp;quot;diff1_acf1&amp;quot;      &amp;quot;diff1_acf10&amp;quot;     &amp;quot;diff2_acf1&amp;quot;     
## [16] &amp;quot;diff2_acf10&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first vector has a high value in &lt;strong&gt;spike&lt;/strong&gt; (0.9172334), which measures the amount of spikiness in a time series. Now, let’s have a look at the 21st window of the time series.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Make a dataframe from window 21
df2 &amp;lt;- cbind.data.frame((1000 + 1:50), my_data_list[[inds]])
colnames(df2) &amp;lt;- c(&amp;quot;Index&amp;quot;, &amp;quot;Value&amp;quot;)
ggplot(df2, aes(Index, Value)) + geom_point() + geom_line() + theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-11-16-using-dobin-for-time-series/index_files/figure-html/analysis3-1.png&#34; width=&#34;672&#34; /&gt; We see that we’ve picked up the spike corresponding to position &lt;span class=&#34;math inline&#34;&gt;\(1010\)&lt;/span&gt;, in the 21st window, because &lt;span class=&#34;math inline&#34;&gt;\(1010/50 = 20.2\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-real-example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A Real Example&lt;/h2&gt;
&lt;p&gt;Next we look at a real world example containing the streamflow from Mad River near Springfield, Ohio from 1915- 1960.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(fpp)
library(ggplot2)
library(tsfeatures)
library(dobin)
library(tsdl)

tt &amp;lt;- tsdl[[77]]
autoplot(tt) +  ggtitle(&amp;quot;Mad River near Springfield OH 1915- 1960&amp;quot;) +
  xlab(&amp;quot;Year&amp;quot;) +  ylab(&amp;quot;Streamflow&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-11-16-using-dobin-for-time-series/index_files/figure-html/realEx-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let’s split the time series into non-overlapping windows and compute features as before.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_data_list &amp;lt;- split(tt, rep(1:23, each = 24))
# Compute features of each chunk using tsfeatues
ftrs &amp;lt;- tsfeatures(my_data_list)

ftrs[ ,4:7] %&amp;gt;% dobin() -&amp;gt; out

coords &amp;lt;- as.data.frame(out$coords[ ,1:2])
colnames(coords) &amp;lt;- c(&amp;quot;DC1&amp;quot;, &amp;quot;DC2&amp;quot;)
ggplot(coords, aes(DC1, DC2)) + geom_point(size=2) + theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-11-16-using-dobin-for-time-series/index_files/figure-html/feat2-1.png&#34; width=&#34;672&#34; /&gt; We see a point having a DC1 value greater than 1. Let us investigate that point.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ind &amp;lt;- which(coords[ ,1] &amp;gt; 1)
ind&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 12&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;- cbind.data.frame((11*24+1):(12*24), my_data_list[[ind]])
colnames(df) &amp;lt;- c(&amp;quot;Index&amp;quot;, &amp;quot;Streamflow&amp;quot;)
ggplot(df, aes(Index, Streamflow)) + geom_point() + geom_line()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-11-16-using-dobin-for-time-series/index_files/figure-html/dobin2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see this point corresponds to the window with the highest spike in the time series, as this is the only spike greater than 75 units.&lt;/p&gt;
&lt;p&gt;So, in summary &lt;em&gt;dobin&lt;/em&gt; can be used as a dimension reduction technique for outlier detection for time series data, as long as the data is prepared appropriately.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
